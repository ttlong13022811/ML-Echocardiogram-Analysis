{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Echo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iij-it9Nxjq9"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "from collections import namedtuple\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkCrIofWyKZd"
      },
      "source": [
        "# **Clone git để lấy dữ liệu**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVk5r8cJxwby",
        "outputId": "61d3ab45-257c-4521-8f40-55e3b9b144e1"
      },
      "source": [
        "!git clone https://github.com/ttlong13022811/ML-Echocardiogram-Analysis\n",
        "traindir = \"/content/ML-Echocardiogram-Analysis/DATA_CHAMBER_2021/train\"\n",
        "testdir = \"/content/ML-Echocardiogram-Analysis/DATA_CHAMBER_2021/test\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ML-Echocardiogram-Analysis'...\n",
            "remote: Enumerating objects: 8364, done.\u001b[K\n",
            "remote: Total 8364 (delta 0), reused 0 (delta 0), pack-reused 8364\u001b[K\n",
            "Receiving objects: 100% (8364/8364), 488.03 MiB | 35.34 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "Checking out files: 100% (8328/8328), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRql0tV8yPC1"
      },
      "source": [
        "# **Chuẩn bị dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hp5tVN-gxyTu"
      },
      "source": [
        "TrainTest = namedtuple('TrainTest', ['train', 'test'])\n",
        "\n",
        "def get_classes():\n",
        "  classes = ['2C', '3C', '4C']\n",
        "  return classes\n",
        "\n",
        "def resize_crop_test(size):\n",
        "  resize = transforms.Resize((size,size))\n",
        "  crop = transforms.RandomCrop((size,size),padding = 4)\n",
        "  test = transforms.Compose([resize,transforms.ToTensor()])\n",
        "  return resize,crop,test\n",
        "\n",
        "def raw(size):\n",
        "  resize,crop,test = resize_crop_test(size)\n",
        "  train_raw = transforms.Compose([resize,\n",
        "                                  transforms.ToTensor()])\n",
        "  return train_raw\n",
        "\n",
        "def aug(size):\n",
        "  resize,crop,test = resize_crop_test(size)\n",
        "  train_aug = transforms.Compose([resize,\n",
        "                                  crop,\n",
        "                                  transforms.RandomHorizontalFlip(),\n",
        "                                  transforms.RandomVerticalFlip(),\n",
        "                                  transforms.ToTensor()])\n",
        "  return train_aug\n",
        "\n",
        "def pre(size):\n",
        "  resize,crop,test = resize_crop_test(size)\n",
        "  train_pre = transforms.Compose([resize,\n",
        "                                  transforms.GaussianBlur(2),\n",
        "                                  transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
        "                                  transforms.ToTensor()])\n",
        "  return train_pre\n",
        "\n",
        "def prepare_data(size):\n",
        "  train = raw(size)\n",
        "  resize,crop,test = resize_crop_test(size)\n",
        "  trainset = torchvision.datasets.ImageFolder(root = traindir, transform = train)\n",
        "  testset = torchvision.datasets.ImageFolder(root = testdir , transform = test)\n",
        "  return TrainTest(train=trainset, test=testset)\n",
        "\n",
        "def prepare_loader(datasets):\n",
        "  trainloader = DataLoader(dataset=datasets.train, batch_size=35, shuffle=True, num_workers=4)\n",
        "  testloader = DataLoader(dataset=datasets.test, batch_size=35, shuffle=False, num_workers=4)\n",
        "  return TrainTest(train=trainloader, test=testloader)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUdqlwzOyWTr"
      },
      "source": [
        "# **Train, Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UxPEz1ax2o9"
      },
      "source": [
        "def train_epoch(epoch, model, loader, loss_func, optimizer, device):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  reporting_steps = 32\n",
        "  for i, (images, labels) in enumerate(loader):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    outputs = model(images)\n",
        "    loss = loss_func(outputs, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "    if i % reporting_steps == reporting_steps-1:\n",
        "      print(f\"Epoch {epoch} step {i} ave_loss {running_loss/reporting_steps:0.4f}\")\n",
        "      running_loss = 0.0\n",
        "\n",
        "def test_epoch(model, loader, device):\n",
        "  ytrue = []\n",
        "  ypred = []\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    for i, (images, labels) in enumerate(loader):\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs, dim=1)\n",
        "      ytrue += list(labels.cpu().numpy())\n",
        "      ypred += list(predicted.cpu().numpy())\n",
        "\n",
        "  return ypred, ytrue"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY79Kaozyce2"
      },
      "source": [
        "# **Main**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDOk_2Aex6_v"
      },
      "source": [
        "def main(model = 'vgg16', size = 32):\n",
        "  classes = get_classes()\n",
        "  datasets = prepare_data(size)\n",
        "  loaders = prepare_loader(datasets)\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  if model == 'vgg16':\n",
        "    print(\"vgg16\")\n",
        "    model = torchvision.models.vgg16()\n",
        "    model.classifier[6] = torch.nn.modules.linear.Linear(in_features=4096, out_features=3, bias=True)\n",
        "  elif model == 'resnet50':\n",
        "    print(\"resnet50\")\n",
        "    model = torchvision.models.resnet50()\n",
        "    model.fc = torch.nn.modules.linear.Linear(in_features=2048, out_features=3, bias=True) \n",
        "  elif model == 'densenet':\n",
        "    print(\"densenet\")\n",
        "    model = torchvision.models.densenet161()\n",
        "    model.classifier = torch.nn.modules.linear.Linear(in_features=1024, out_features=3, bias=True)\n",
        "\n",
        "  model.to(device)\n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "  for epoch in range(10):\n",
        "    train_epoch(epoch, model, loaders.train, loss_func, optimizer, device)\n",
        "    ypred, ytrue = test_epoch(model, loaders.test, device)\n",
        "    print(classification_report(ytrue, ypred, target_names=classes))\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOMdrP-zyAxE",
        "outputId": "335e2276-369d-408b-9fb5-4f9755824ec2"
      },
      "source": [
        "main('vgg16',32)  "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vgg16\n",
            "Epoch 0 step 31 ave_loss 1.0983\n",
            "Epoch 0 step 63 ave_loss 1.0965\n",
            "Epoch 0 step 95 ave_loss 1.0762\n",
            "Epoch 0 step 127 ave_loss 1.0855\n",
            "Epoch 0 step 159 ave_loss 1.0964\n",
            "Epoch 0 step 191 ave_loss 1.0999\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.25      1.00      0.41       409\n",
            "          3C       0.00      0.00      0.00       367\n",
            "          4C       0.00      0.00      0.00       831\n",
            "\n",
            "    accuracy                           0.25      1607\n",
            "   macro avg       0.08      0.33      0.14      1607\n",
            "weighted avg       0.06      0.25      0.10      1607\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 step 31 ave_loss 1.0963\n",
            "Epoch 1 step 63 ave_loss 1.0982\n",
            "Epoch 1 step 95 ave_loss 1.0977\n",
            "Epoch 1 step 127 ave_loss 1.0961\n",
            "Epoch 1 step 159 ave_loss 1.0901\n",
            "Epoch 1 step 191 ave_loss 1.0718\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.29      1.00      0.45       409\n",
            "          3C       0.00      0.00      0.00       367\n",
            "          4C       1.00      0.25      0.40       831\n",
            "\n",
            "    accuracy                           0.39      1607\n",
            "   macro avg       0.43      0.42      0.29      1607\n",
            "weighted avg       0.59      0.39      0.32      1607\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 step 31 ave_loss 0.9733\n",
            "Epoch 2 step 63 ave_loss 0.7274\n",
            "Epoch 2 step 95 ave_loss 0.4851\n",
            "Epoch 2 step 127 ave_loss 0.3552\n",
            "Epoch 2 step 159 ave_loss 0.3032\n",
            "Epoch 2 step 191 ave_loss 0.1688\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.64      0.87      0.73       409\n",
            "          3C       0.81      0.90      0.85       367\n",
            "          4C       1.00      0.77      0.87       831\n",
            "\n",
            "    accuracy                           0.82      1607\n",
            "   macro avg       0.81      0.85      0.82      1607\n",
            "weighted avg       0.86      0.82      0.83      1607\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 step 31 ave_loss 0.2924\n",
            "Epoch 3 step 63 ave_loss 0.1586\n",
            "Epoch 3 step 95 ave_loss 0.1341\n",
            "Epoch 3 step 127 ave_loss 0.0830\n",
            "Epoch 3 step 159 ave_loss 0.1504\n",
            "Epoch 3 step 191 ave_loss 0.0792\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.58      0.97      0.73       409\n",
            "          3C       0.86      0.78      0.82       367\n",
            "          4C       0.98      0.70      0.82       831\n",
            "\n",
            "    accuracy                           0.79      1607\n",
            "   macro avg       0.81      0.82      0.79      1607\n",
            "weighted avg       0.85      0.79      0.80      1607\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 step 31 ave_loss 0.1019\n",
            "Epoch 4 step 63 ave_loss 0.0678\n",
            "Epoch 4 step 95 ave_loss 0.0234\n",
            "Epoch 4 step 127 ave_loss 0.0127\n",
            "Epoch 4 step 159 ave_loss 0.0428\n",
            "Epoch 4 step 191 ave_loss 0.0328\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.79      0.80      0.80       409\n",
            "          3C       0.60      0.94      0.73       367\n",
            "          4C       1.00      0.74      0.85       831\n",
            "\n",
            "    accuracy                           0.80      1607\n",
            "   macro avg       0.80      0.83      0.79      1607\n",
            "weighted avg       0.86      0.80      0.81      1607\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 step 31 ave_loss 0.0113\n",
            "Epoch 5 step 63 ave_loss 0.0249\n",
            "Epoch 5 step 95 ave_loss 0.0277\n",
            "Epoch 5 step 127 ave_loss 0.0100\n",
            "Epoch 5 step 159 ave_loss 0.0160\n",
            "Epoch 5 step 191 ave_loss 0.0156\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.88      0.85      0.87       409\n",
            "          3C       0.64      0.93      0.76       367\n",
            "          4C       1.00      0.81      0.90       831\n",
            "\n",
            "    accuracy                           0.85      1607\n",
            "   macro avg       0.84      0.87      0.84      1607\n",
            "weighted avg       0.89      0.85      0.86      1607\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 step 31 ave_loss 0.0091\n",
            "Epoch 6 step 63 ave_loss 0.0047\n",
            "Epoch 6 step 95 ave_loss 0.0107\n",
            "Epoch 6 step 127 ave_loss 0.0368\n",
            "Epoch 6 step 159 ave_loss 0.0072\n",
            "Epoch 6 step 191 ave_loss 0.0244\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.87      0.94      0.90       409\n",
            "          3C       0.67      0.89      0.76       367\n",
            "          4C       0.99      0.81      0.89       831\n",
            "\n",
            "    accuracy                           0.86      1607\n",
            "   macro avg       0.84      0.88      0.85      1607\n",
            "weighted avg       0.89      0.86      0.87      1607\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 step 31 ave_loss 0.0023\n",
            "Epoch 7 step 63 ave_loss 0.0001\n",
            "Epoch 7 step 95 ave_loss 0.0002\n",
            "Epoch 7 step 127 ave_loss 0.0002\n",
            "Epoch 7 step 159 ave_loss 0.0015\n",
            "Epoch 7 step 191 ave_loss 0.0001\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.92      0.88      0.90       409\n",
            "          3C       0.61      0.94      0.74       367\n",
            "          4C       1.00      0.78      0.88       831\n",
            "\n",
            "    accuracy                           0.84      1607\n",
            "   macro avg       0.84      0.87      0.84      1607\n",
            "weighted avg       0.89      0.84      0.85      1607\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 step 31 ave_loss 0.0001\n",
            "Epoch 8 step 63 ave_loss 0.0001\n",
            "Epoch 8 step 95 ave_loss 0.0000\n",
            "Epoch 8 step 127 ave_loss 0.0001\n",
            "Epoch 8 step 159 ave_loss 0.0001\n",
            "Epoch 8 step 191 ave_loss 0.0001\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.91      0.89      0.90       409\n",
            "          3C       0.62      0.94      0.75       367\n",
            "          4C       1.00      0.78      0.88       831\n",
            "\n",
            "    accuracy                           0.85      1607\n",
            "   macro avg       0.84      0.87      0.84      1607\n",
            "weighted avg       0.89      0.85      0.85      1607\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 step 31 ave_loss 0.0001\n",
            "Epoch 9 step 63 ave_loss 0.0001\n",
            "Epoch 9 step 95 ave_loss 0.0000\n",
            "Epoch 9 step 127 ave_loss 0.0001\n",
            "Epoch 9 step 159 ave_loss 0.0001\n",
            "Epoch 9 step 191 ave_loss 0.0001\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.91      0.89      0.90       409\n",
            "          3C       0.62      0.94      0.75       367\n",
            "          4C       1.00      0.79      0.88       831\n",
            "\n",
            "    accuracy                           0.85      1607\n",
            "   macro avg       0.84      0.87      0.84      1607\n",
            "weighted avg       0.89      0.85      0.86      1607\n",
            "\n"
          ]
        }
      ]
    }
  ]
}